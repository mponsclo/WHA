---
title: "World Happiness"
author: "Manuel Breve, Diego Quintana, Marcel Pons"
date: "06/07/2020"
output:
  html_document:
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
# imports for the project
library(mice) # for imputing missing data
library(psych)
library(ggplot2) # for an improved toolset for plots
library(tableplot)
library(reshape2)
library(chemometrics) # for the mahalanobis distance
library(pracma)
library(PerformanceAnalytics) # to plot histograms
library(dplyr) # for an improved toolset
library(tibble) # rownames_to_column and viceversa

# clustering
library(FactoMineR)
library(factoextra)
library(plotrix)
library(gplots)
library(tableplot)

#Trees
library(rpart)
library(rpart.plot)
library(randomForest)
library(rsample)

set.seed(42)
```

### Loading World Happiness Datasets 

```{r}
path2018 <- "./WHR2018.csv"
path2019 <- "./WHR2019.csv"

df_row_n <- read.csv(path2018, header = TRUE, sep = ",", dec=".",na.strings="N/A")[,-1] # Overall rank skipped (cities in order)

test_row_n <- read.csv(path2019, header = TRUE, sep = ",", dec=".",na.strings="N/A")[,-1] # Overall rank skipped (cities in order)

rm(path2018,path2019)
summary(df_row_n)
head(df_row_n)
```

Imputing missing values:

```{r Remove 1 NA, include=FALSE, echo=FALSE}
impute <- mice(df_row_n, m=5, seed=500)
df_row_n <- mice::complete(impute, 1)
rm(impute)
```

```{r df with rownames, include=FALSE}
df <- column_to_rownames(df_row_n, 'Country.or.region')
test <- column_to_rownames(test_row_n, 'Country.or.region')
```

Check Correlations

```{r, echo=FALSE}
chart.Correlation(df[,-c(8)], histogram=F)
```

### Assessing Distributions

Checking if all the marginals are univariate normal
```{r Density Distributions, echo=FALSE}
#str(df)
distdat <- function(z, titlename,dens) {
  ggplot(df, aes(x=df[,z])) + geom_histogram(aes(y=..density..), bins=25, colour = "black", fill = "white") + 
  geom_density(alpha=.2,  fill="#FF6666") + xlab("") + ylab(dens) + ggtitle(titlename) + theme_bw() + theme(plot.title = element_text(hjust = 0.5)) + scale_x_continuous(breaks = NULL)
}

a1 <-  distdat(2, "GDP per capita","Density")
a2 <- distdat(3, "Social Support","")
a3 <- distdat(4, "Healthy life expect.","")
a4 <- distdat(5, "Choice Freedom","")
a5 <- distdat(6, "Generosity","Density")
a6 <- distdat(7, "Perc. Corruption","")
a7 <- distdat(1, "Score","")

gridExtra::grid.arrange(a1,a2,a3,a4,a5,a6,a7, nrow=2, ncol=4)
rm(a1,a2,a3,a4,a5,a6,a7, distdat)
```

```{r QQPlot, echo=FALSE}
par(mfrow=c(2,4))

qqnorm(df$GDP.per.capita, col = "darkred", main = "GDP")
qqline(df$GDP.per.capita, col = "darkblue", lwd = 3)

qqnorm(df$Social.support, col = "darkred", main = "Social Supp.")
qqline(df$Social.support, col = "darkblue", lwd = 3)

qqnorm(df$Healthy.life.expectancy, col = "darkred", main = "Life Exp.")
qqline(df$Healthy.life.expectancy, col = "darkblue", lwd = 3)

qqnorm(df$Freedom.to.make.life.choices, col = "darkred", main = "C Freedom")
qqline(df$Freedom.to.make.life.choices, col = "darkblue", lwd = 3)

qqnorm(df$Generosity, col = "darkred", main = "Generosity")
qqline(df$Generosity, col = "darkblue", lwd = 3)

qqnorm(df$Perceptions.of.corruption, col = "darkred", main = "P Corruption")
qqline(df$Perceptions.of.corruption, col = "darkblue", lwd = 3)

qqnorm(df$Score, col = "darkred", main = "Ladder Score")
qqline(df$Score, col = "darkblue", lwd = 3)
```
```{r Shapiro Tests, include=FALSE}
shapiro.test(df$GDP.per.capita) #p-value < 0.05
shapiro.test(df$Social.support) #p-value < 0.05
shapiro.test(df$Healthy.life.expectancy) #p-value < 0.05
shapiro.test(df$Freedom.to.make.life.choices) #p-value < 0.05
shapiro.test(df$Generosity) #p-value < 0.05
shapiro.test(df$Perceptions.of.corruption) #p-value < 0.05
shapiro.test(df$Score) #p-value > 0.05 NORMALITY 
```

#### Checking if the Mahalanobis distances to the centroid follow a Chi-Squared Distribution
```{r Distribution Mahalanobis Distances}
X.cov <- var(scale(df[,-8])) # standardize first df[-c(20),-8]
#X.cov <- var(pcs)
X.mean <- apply(df[,-8],2,mean)
normal_dist <- mahalanobis(scale(df[,-8]), X.mean, X.cov) # We take out the Unite Emirates point that is further away

densityplot(normal_dist)
n <- length(normal_dist)
# QQ Plot of the Mahalanobis Distances
emp_cum <- (c(1:n)-0.5)/n
plot((mean(normal_dist))*sort(normal_dist)/(2*mean(normal_dist)), qchisq(emp_cum, df = mean(normal_dist)), pch=20, main="Q-Q plot Mahabanobis Distances", xlab="Observed",ylab="Theoretical")
aa=qchisq(emp_cum, df = mean(normal_dist))
bb=(mean(normal_dist))*sort(normal_dist)/(2*mean(normal_dist))
abline(lm(aa~bb), col="blue")

# Simulation of Theoretical Chi-Square distribition with df=155
(cc.obs <- cor(sort(normal_dist), qchisq(emp_cum, df = (6))))
cc.teo <- NULL # THEORETIC CORRELATIONS OF A CHISQUARE
niter <- 1000
for (k in 1:niter){
  aa <- rchisq(n, df=(mean(normal_dist))) # n=156
  cc.teo <-  c(cc.teo, cor(sort(aa), qchisq(emp_cum, df=mean(normal_dist))))
}

#plot(density(cc.teo)) # plot of the null hypotesis

# P.VALUE OF THE OBSERVED CORRELATION 
(p.val_cc.obs <- sum(cc.teo < cc.obs)/niter) #ACCEPT - follows a Chi-square distribution

rm(aa, bb, cc.obs, cc.teo, emp_cum, k, n, niter, normal_dist, p.val_cc.obs)
```

### Outliers 
#### Univaritare Outliers 
```{r Boxplots, echo=FALSE}
gplot <- function(z){
  ggplot(data=df, aes(x=factor(''), y=df[,z])) + geom_boxplot(fill="azure3", outlier.color = "red", outlier.shape = 1) + theme(legend.position='none') + labs(title = paste(colnames(df)[z]), x="",y="")
}
p1 <- gplot(2)
p2 <- gplot(3)
p3 <- gplot(4)
p4 <- gplot(5)
p5 <- gplot(6)
p6 <- gplot(7)
p7 <- gplot(1)

gridExtra::grid.arrange(p1,p2,p3,p4,p5,p6,p7, nrow=3, ncol=3)
rm(p1,p2,p3,p4,p5,p6,p7, gplot)
```

```{r UNIVARIATE OUTLIERS}
uni.outlier <-function(z, mod=c("extreme","mild")){
  mod <- match.arg(mod)
  if (mod == "extreme") {
      extremelower <- as.numeric(quantile(df[,z],0.25)-(3*IQR(df[,z])))
      extremeupper <- as.numeric(quantile(df[,z],0.75)+(3*IQR(df[,z])))
      outliers <- df %>% rownames_to_column('city') %>%
        filter(df[,z]<extremelower | df[,z]>extremeupper) %>%
        column_to_rownames('city')
      return(outliers)
  } else if (mod == "mild") {
    extremelower <- as.numeric(quantile(df[,z],0.25)-(1.5*IQR(df[,z])))
    extremeupper <- as.numeric(quantile(df[,z],0.75)+(1.5*IQR(df[,z])))
    outliers <- df %>% rownames_to_column('city') %>%
      filter(df[,z]<extremelower | df[,z]>extremeupper) %>%
      column_to_rownames('city')
    return(outliers)
  } 
} 

gdp.uniout <- uni.outlier(2,mod = "extreme") # No outliers
gdp.uniout1 <- uni.outlier(2,mod = "mild")
support.uniout <- uni.outlier(3,mod = "extreme")  # No outliers
(support.uniout1 <- uni.outlier(3,mod = "mild")) 
health.uniout <- uni.outlier(4,mod = "extreme")   # No outliers
health.uniout1 <- uni.outlier(4,mod = "mild")     # No outliers
free.uniout <- uni.outlier(5,mod = "extreme")     # No outliers
(free.uniout1 <- uni.outlier(5,mod = "mild"))
gene.uniout <- uni.outlier(6,mod = "extreme")     # No outliers
(gene.uniout1 <- uni.outlier(6,mod = "mild"))
(corr.uniout <- uni.outlier(7,mod = "extreme"))
(corr.uniout1 <- uni.outlier(7,mod = "mild"))
ladder.uniout <- uni.outlier(1,mod = "extreme")   # No outliers
ladder.uniout1 <- uni.outlier(1,mod = "mild")    # No outliers


rm(gdp.uniout, gdp.uniout1, support.uniout, support.uniout1, health.uniout, health.uniout1, free.uniout, free.uniout1, gene.uniout, gene.uniout1, corr.uniout, corr.uniout1, ladder.uniout, ladder.uniout1)
```

#### Multivariate Outliers
```{r MULITIVARIATE AND DISTANCE DISTRIBUTIONS}
#Mild univariate maybe aren't multivariate outliers
#Removing extreme ouliers: Denmark, Singapore, Rwanda --> The are also outliers in 2019 and 2020
remove <- c('Denmark','Singapore','Rwanda')
dataset <- df[!row.names(df)%in%remove,]

outliers <- Moutlier(dataset[,-c(8)], quantile = 0.999, plot = TRUE)

p <- densityplot(outliers$md, xlab="Classical Distances")
p2 <- densityplot(outliers$rd, xlab= "Robust Distances")
gridExtra::grid.arrange(p,p2, nrow=2, ncol=1)
```

```{r include=FALSE}
distances <- outliers$md
isOutlier <- (distances >= outliers$cutoff)

robust_distances <- outliers$rd
is_robust_Outlier <- (robust_distances >= outliers$cutoff)

table(isOutlier)
table(is_robust_Outlier)
```

```{r}
#Check Multivariate Outliers
out <- data.frame(data.frame(isOutlier),row.names = row.names(dataset))
subset(out, isOutlier == 'TRUE')
rm(outliers, out, isOutlier,is_robust_Outlier,robust_distances, uni.outlier, distances)
```

***
### PRINCIPAL COMPONENT ANALYSIS
```{r PCA, include=FALSE}
#PCA with outliers as ind.sup
supplementary_individuals <- c('Denmark','Singapore', 'Rwanda', "United Arab Emirates")
(out_idx <- which(rownames(df) %in% supplementary_individuals))

#dataset_without_outliers <- dataset[!row.names(dataset)%in%supplementary_individuals,]
res.pca <- PCA(df, quanti.sup = 1, quali.sup = 8,ind.sup = c(3,20,34,151),  scale.unit = TRUE, graph = FALSE)
```

```{r Plots 1st Factorial Plane, echo=FALSE}
fviz_pca_ind(res.pca, habillage = df[-c(3,20,34,151),]$Regional.Indicator, col.ind.sup = "black") +  
             theme(plot.title = element_text(hjust = 0.5),
                   panel.grid.major = element_blank(),
                   panel.grid.minor = element_blank(),
                   panel.background = element_rect(fill = "white"),
                   plot.background = element_blank(),
                   axis.line = element_line(colour = "black"),
                   legend.position = 'bottom') + xlim(-5.5, 5.5)

fviz_pca_var(res.pca, col.var="orangered2",repel=TRUE) +
             theme(plot.title = element_text(hjust = 0.5), 
                   panel.grid.major = element_blank(), 
                   panel.grid.minor = element_blank(), 
                   panel.background = element_rect(fill="white"),
                   plot.background = element_blank(), 
                   axis.line = element_line(colour = "black"), 
                   legend.position = 'bottom')
```

```{r Info PCA (plots),  echo=FALSE}
# res.pca$var$cor
# QUALITY OF VARIABLES 
vars <- data.frame(res.pca$var$cos2)
vars["Representation"] = round((vars$Dim.1 + vars$Dim.2),2)
(p <- ggplot(data=vars, aes(x=rownames(vars), y=Representation,  label=Representation)) +
      geom_bar(stat="identity",fill="cornflowerblue", color='black') + 
      xlab("Variable") + ylab("Cos2 Dim1 + Cos2 Dim2") + 
      ggtitle("Representation of variables in first factorial plane") +
      geom_text(aes(label=Representation)) + geom_label() + 
      theme(plot.title = element_text(hjust = 0.5), 
            panel.grid.major = element_blank(), 
            panel.grid.minor = element_blank(), 
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black"), legend.position = 'none'))

# CONTRIBUTION OF VARIABLES 
p1 <- fviz_contrib(res.pca, choice = "var",fill="cornflowerblue",color = "steelblue", axes=1)
p2 <- fviz_contrib(res.pca, choice = "var",fill="cornflowerblue",color = "steelblue", axes=2, ylabel="")
gridExtra::grid.arrange(p1,p2, nrow=1, ncol=2)
rm(vars, p, p1, p2)

# CONTRIBUTION INDIVIDUALS 
p1 <- fviz_contrib(res.pca, choice = "ind",fill="cornflowerblue",color = "steelblue", top=10, axes=1)
p2 <- fviz_contrib(res.pca, choice = "ind",fill="cornflowerblue",color = "steelblue", top=10, axes=2)
gridExtra::grid.arrange(p1,p2, nrow=2, ncol=1)

# QUALITY INDIVIDUALS 
p1 <- fviz_cos2(res.pca, choice = "ind",fill="cornflowerblue",color = "steelblue", top=10, axes=1)
p2 <- fviz_cos2(res.pca, choice = "ind",fill="cornflowerblue",color = "steelblue", top=10, axes=2)
gridExtra::grid.arrange(p1,p2, nrow=2, ncol=1)

# SUPPLEMENTARY INTERPRETATION
res.pca$ind.sup$cos2 # Individuals 
res.pca$quali.sup$v.test # Quali.sup <- Regional indicator
res.pca$quali.sup$cos2 # Quali.sup <- Regional indicator
res.pca$quali.sup$eta2 # Quali.sup <- Regional indicator
res.pca$quanti.sup$cor # Quanti.sup <- Ladder.score
res.pca$quanti.sup$cos2 # Quanti.sup <- Ladder.score
```

#### Biplot
```{r Biplot, echo=FALSE}
fviz_pca_biplot(res.pca, geom = c("point", "text"),
                col.var="orangered4",col.ind.sup="black",col.quanti.sup="orangered2", 
                title="Biplot", habillage=df[-c(3,20,34,151),]$Regional.Indicator) + 
                theme(plot.title = element_text(hjust = 0.5), 
                      panel.grid.major = element_blank(), 
                      panel.grid.minor = element_blank(), 
                      panel.background = element_blank(), 
                      axis.line = element_line(colour = "black"), 
                      legend.position = "bottom") + xlim(-6.5,6.5)
```

```{r Screeplot, echo=FALSE}
fviz_screeplot(res.pca, 
               addlabels = TRUE, 
               ggtheme = theme_minimal(), 
               barfill= 'cornflowerblue', 
               barcolor= 'cornflowerblue') + 
               theme(plot.title = element_text(hjust = 0.5), 
                     panel.grid.major = element_blank(), 
                     panel.grid.minor = element_blank(), 
                     panel.background = element_blank(), 
                     axis.line = element_line(colour = "black"))
```
***
#### VARIMAX ROTATION AND LATENT FACTORS 
```{r VARIMAX, echo=F}
#5 significant dimenions
significant_dimensions <- 5

#Merging coordinates of individuals
coordinates <- res.pca$ind$coord[,1:significant_dimensions]
coordinates_ind.sup <- res.pca$ind.sup$coord[,1:significant_dimensions]
coordinates <- data.frame(coordinates)
coordinates_ind.sup <- data.frame(coordinates_ind.sup)

#Psi and Phi
psi <- rbind(coordinates, coordinates_ind.sup)
Phi <- res.pca$var$coord[,1:significant_dimensions]

#Varimax
pc.rot <- varimax(Phi)
pc.rot$rotmat
pc.rot$loadings

#First Factor is about Logged.GDP.per.capita, Social.support, Healthy.life.expectancy 
#Seconf Factor is about generosity
#Third Factor is about Dystopia...residual
#Fourth Factor is about Perceptions.of.corruption
#Fifth Factor is about Freedom.to.make.life.choices
```

```{r Factors Variables projection, echo=FALSE}
#Factors Variables projection
p_var = cbind(pc.rot$loadings[,1:2])
p_var <- as.data.frame(p_var)

var_plot <- ggplot() + theme(aspect.ratio=1) + theme_bw(base_size = 20) 
angle <- seq(-pi, pi, length = 50) 
circle <- data.frame(x = sin(angle), y = cos(angle))

var_plot <- var_plot + geom_path(aes(x, y), data = circle, color="black") 
var_plot <- var_plot + geom_text(data=p_var, aes(x=Dim.1,y=Dim.2,label=rownames(p_var))) 
var_plot <- var_plot + ggtitle("Varimax rotation, variables projection") + xlab("PC1") + ylab("PC2")
var_plot <- var_plot + xlim(-1.2,1) + ylim(-1,1) + geom_label()
var_plot <- var_plot + geom_segment(data=p_var, aes(x = 0, y = 0, xend = Dim.1, yend = Dim.2), arrow = arrow(), color = "orangered2") 
var_plot <- var_plot + geom_hline(yintercept = 0, colour = "black", linetype=2) + geom_vline(xintercept = 0, colour = "black", linetype=2) + theme_bw() + coord_fixed()
var_plot <- var_plot + 
            theme(plot.title = element_text(hjust = 0.5),  
                  panel.grid.major = element_blank(), 
                  panel.grid.minor = element_blank(), 
                  panel.background = element_blank())
var_plot
rm(angle,circle)
```

```{r Individuals projection, echo=FALSE}
#Individuals projection
p_ind <- cbind(as.matrix(psi)%*%pc.rot$rotmat)
p_ind <- as.data.frame(p_ind)
p_ind <- p_ind[c(1,2)]
country_region <- df[c(8)]
p_ind <- as.data.frame(merge(p_ind,country_region, by=0))

colnames(p_ind) <- c('Country','PC1', 'PC2','Region')

ccolors=c("black","lightslateblue","mediumorchid1","firebrick1","green","lightcoral","deeppink","gray55","darkgoldenrod1","darkblue")

varimax_ind <- ggplot(p_ind, aes(x = PC1, y = PC2, fill=Region, label=Country), color=Region) +
              geom_point(aes(color=Region))

varimax_ind <- varimax_ind + theme_bw() + 
               theme(legend.position="bottom", 
                     legend.text = element_text(size=6), 
                     plot.title = element_text(hjust=0.5))

varimax_ind <- varimax_ind + 
               geom_text(aes(color = Region), 
                         check_overlap = F, 
                         vjust=-0.5, alpha=0.5, 
                         show.legend = F)
varimax_ind <- varimax_ind + scale_colour_manual(values=ccolors)

varimax_ind <- varimax_ind + xlab("Rotated PC1") + ylab("Rotated PC2") 
varimax_ind <- varimax_ind + ggtitle("Varimax rotation, individuals projection")

varimax_ind <- varimax_ind + geom_hline(yintercept = 0, linetype= 2, colour = "grey60") +
               geom_vline(xintercept = 0, linetype=2,colour = "grey60") + 
               theme(plot.title = element_text(hjust = 0.5), 
                     panel.background = element_blank(), 
                     axis.line = element_line(colour = "black"))
varimax_ind

#rm(angle, ccolors, varimax_ind, var_plot,pc.rot,p_var,p_ind, circle, coordinates, coordinates_ind.sup, country_region)
```

***
#### CLUSTERING 
```{r Cluster 1, echo=FALSE}
hcpc <- HCPC(psi[-c(153:156),] ,nb.clust = -1, consol = F, graph = FALSE)
barplot(hcpc$call$t$inert.gain[1:20])
```

```{r Consolidation, echo=FALSE}
## 5
nc <- 5
final_hcpc <- HCPC (psi[-c(153:156),], nb.clust = nc, graph = F)
```

```{r Dendogram}
fviz_dend(final_hcpc,
          k = 5,
          cex = 0.7,                     # Label size
          palette = "jco",               # Color palette see ?ggpubr::ggpar
          rect = TRUE, rect_fill = TRUE, # Add rectangle around groups
          rect_border = "jco",           # Rectangle color
          )
```

```{r Catdes, echo=FALSE}
cut <- final_hcpc$data.clust$clust
table(cut)
catdes(cbind(as.factor(cut),df[-c(3,20,34,151),]),1,proba = 0.0001) # Actual variables
```
```{r Parangons}
final_hcpc$desc.ind$para
```

```{r echo=FALSE}
clusters <- cbind(cut,psi[-c(153:156),])
```

```{r include=FALSE}
clusters$cluster[clusters$cut==1] <- "Cluster 1"
clusters$cluster[clusters$cut==2] <- "Cluster 2"
clusters$cluster[clusters$cut==3] <- "Cluster 3"
clusters$cluster[clusters$cut==4] <- "Cluster 4"
clusters$cluster[clusters$cut==5] <- "Cluster 5"
colnames(clusters) <- c('Cut', 'Dim.1', 'Dim.2','Dim.3','Dim.4', 'Dim.5', 'Cluster')

```

```{r include=FALSE}
parangons_clusters <- clusters[c(5, 6, 11, 13, 14,
                                 25, 26, 38, 39, 45,
                                 63, 75, 78, 80, 89, 
                                 96, 98, 101, 104, 113, 
                                 119, 126, 131, 137, 146),]
Clusters <- clusters$cluster
```

```{r Cluster Plot, echo=FALSE}
ccolors=c("black","lightslateblue","mediumorchid1","firebrick1","green","lightcoral","deeppink","gray55","darkgoldenrod1","darkblue")

plot <- ggplot(data = clusters, aes(x = Dim.1, y = Dim.2)) +
        geom_hline(yintercept = 0, linetype=2, colour = "black") +
        geom_vline(xintercept = 0, linetype=2, colour = "black") +
        geom_text(aes(label=rownames(clusters), colour = Cluster), alpha = 0.5) +
        geom_label(aes(label=rownames(parangons_clusters), 
                       colour = Cluster), alpha = 0.5, 
                       data=parangons_clusters) +
        labs(x = "Dim 1", y = "Dim 2") + scale_colour_manual(values=ccolors) + xlim(-7,7) +
        ggtitle("Projection of the individuals (after consolidation)") + theme_bw() +
        theme(plot.title = element_text(hjust = 0.5),  
              panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank())

plot
rm(ccolors,Clusters,cut,nc,significant_dimensions)
```

***
#### DECISION TREE
```{r DECISION TREE}
set.seed(75)
df2 <- df[-c(3,20,34,151),-8] #Removing Outliers and the Region Variable 

df.tree <- rpart(Score ~ ., data=df2,
                 method="anova",
                 control=rpart.control(cp=0.001, xval=10))
rpart.plot(df.tree, roundint = F)
```

```{r COMPLEXITY TABLE}
printcp(df.tree)
cptab <- as.data.frame(df.tree$cptable)
```

```{r}
plotcp(df.tree, minline=TRUE)
```

```{r}
#Tree with the minimum CV error
table_tree <- as.data.frame(df.tree$cptable)
min_error_tree <- which.min(table_tree$xerror)
min_error_tree
```

```{r}
#Postpruning
alpha = table_tree$CP[min_error_tree]
optimum_tree <- prune(df.tree,cp=alpha)
rpart.plot(optimum_tree, roundint = F)
```

```{r PREDICTION ERROR}
#Predict for 2019
predicted <- predict(optimum_tree,newdata = test,type = "vector")
#Mean squared prediction error
mean((test$Score - predict(optimum_tree,newdata = test,type = "vector")) ^ 2)
```

#####  B.2.- Random Forest

```{r}
set.seed(75)
random_forest <- randomForest(formula = Score ~ .,data = df2, type="regression", importance=TRUE)
random_forest
```

```{r}
#Predict Random Forest
predicted_rf <- predict(random_forest, newdata = test , type = "response")
#Mean squared prediction error
mean((test$Score - predict(random_forest, newdata = test, type = "response")) ^ 2)

Metrics::mse(test$Score, predicted_rf)
Metrics::rmse(test$Score, predicted_rf)
Metrics::mae(test$Score, predicted_rf)
Metrics::mape(test$Score, predicted_rf)
```

Tune RF

```{r TUNE RF}
set.seed(42)              
res <- tuneRF(x = df2[,-1],
              y = df2$Score,
              ntreeTry = 2000,
              doBest = TRUE)
               
# Look at results
print(res)
plot(res)
res
```

```{r}
# Generate predicted classes using the model object
pred.rf.tuned <- predict(object = res,   # model object 
                    newdata = test)  # test dataset) # return classification labels


Metrics::mse(test$Score, pred.rf.tuned)
Metrics::rmse(test$Score, pred.rf.tuned)
Metrics::mae(test$Score, pred.rf.tuned)
Metrics::mape(test$Score, pred.rf.tuned)
```

```{r GRID SEARCH}
set.seed(42)
# Establish a list of possible values for mtry, nodesize and sampsize
(mtry <- seq(round(ncol(df2)*0.2), round(ncol(df2)*0.8), 1))
(nodesize <- seq(3, 8, 2))
(sampsize <- round(nrow(df2) * c(0.3, 0.5, 0.7, 0.8)))

# Create a data frame containing all combinations 
(hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize))



# Create an empty vector to store OOB error values 
mse_err <- matrix(ncol = 6, nrow = nrow(hyper_grid))
# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = Score ~ ., 
                          data = df2,
                          mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i],
                          importance = T)
                          
    # Store MSE for the model                      
    mse_err[i, 1] <- mean(model$mse)
    mse_err[i, 2] <- mean(model$rsq)
    mse_err[i, 3] <- mean(model$oob.times)
    mse_err[i, 4] <- hyper_grid$mtry[i]
    mse_err[i, 5] <- hyper_grid$nodesize[i]
    mse_err[i, 6] <- hyper_grid$sampsize[i]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(mse_err[, 1])
print(cbind(mse_err[opt_i, ]))
```

```{r OPTIMAL RF}
model.opt <- randomForest(formula = Score ~ ., 
                          data = df2,
                          mtry = hyper_grid$mtry[opt_i],
                          nodesize = hyper_grid$nodesize[opt_i],
                          sampsize = hyper_grid$sampsize[opt_i],
                          importance = T)
model.opt
```

```{r}
# Generate predicted classes using the model object
pred.rf.opt <- predict(object = model.opt,   # model object 
                    newdata = test)  # test dataset) # return classification labels



Metrics::mse(test$Score, pred.rf.opt)
Metrics::rmse(test$Score, pred.rf.opt)
Metrics::mae(test$Score, pred.rf.opt)
Metrics::mape(test$Score, pred.rf.opt)
```

```{r}
ggplot(
  data = df,
  aes(
    x = GDP.per.capita,
    y = Score,
    size = Generosity,
    color = Healthy.life.expectancy
  )
) + geom_point()
```

