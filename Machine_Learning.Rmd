---
title: "Machine Learning - World Happiness Report"
author: "Manuel Breve, Diego Quintana, Marcel Pons"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
#Libraries
library(mice) #For imputing missing data
library(psych)
library(gplots) #Plots
library(ggplot2) #Plots
library(tableplot) #Plots
library(plotrix) #Plots
library(PerformanceAnalytics) # Plots
library(dplyr) #Toolset
library(reshape2)
library(chemometrics) #Mahalanobis Distance
library(pracma)
library(tibble) #rownames_to_column and viceversa
library(FactoMineR) #PCA
library(factoextra) #PCA
library(cclust) #K-means Clustering
library(Rmixmod) #E-M Clusering
library(Matrix)
library(glmnet) #Linear regression
library(MASS) #Linear regression
library(rpart) #Decision Trees
library(rpart.plot) #Decision Trees
library(randomForest) #Random Forests
library(ROCR) #Metrics
library(caret)
library(neuralnet) # Neural Networks
library(GGally) # Neural Networks
library(boot) #Cross Validation
library(clusterGeneration)
library(nnet)
library(NeuralNetTools)
set.seed(42)
```

***
## Dataset Description

The World Happiness Report is a landmark survey of the state of global happiness that ranks, through a Happiness Score, 156 countries by how happy their citizens perceive themselves to be.

In this project, we analyze the World Happiness Report for the years 2018 and 2019 through different machine learning techniques. We found that the GDP per capita per country is highly correlated with the Happiness Score, and that we can efficiently cluster them into three groups.

Finally we built different machine learning models, both linear and nonlinear, such as linear and polynomial regression, decision trees, random forests and neural networks in order to predict the 2019 Happiness Score, being neural networks the one with the best performance, i.e, the minimum RMSE value.

The World Happiness Dataset is made of one (1) continuous response variable, six (6) continuous independent variables, and one (1) supplementary categorical variable.

#### Continuous Response Variable

+ **Score:** This Happiness Score is a subjective well-being perception based on the survey’s answers, which ask people to evaluate different subjects about their life quality on a scale of 0 to 10. 

#### Continuous Independent Variables:

+ **GDP per capita:** in terms of Purchasing Power Parity (PPP) adjusted to constant 2011 international dollars, taken from the World Development Indicators (WDI) released by the World Bank. (Using natural log of GDP per capita, as this form fits the data significantly better than GDP per capita)
+ **Social support:** the national average of the binary response (either 0 or 1) to the Gallup World Poll (GWP) question “If you are in trouble, do you have relatives or friends you can count on to help you whenever you need them, or not?
+ **Healthy life expectancy at birth:** constructed from the World Health Organization (WHO) and WDI. Adjustment by applying the country-specific ratios to other years.
+ **Freedom to make life choices:** the national average of binary response to the GWP question “Are you satisfied or dissatisfied with your freedom to choose what you do with your life?”
+ **Generosity:** residual of regressing the national average of GWP responses to the question “Have you donated money to a charity in the past month?” on GDP per capita. 
+ **Perceptions of corruption:** the average of binary answers to two GWP questions: “Is corruption widespread throughout the government or not?” and “Is corruption widespread within businesses or not?”. Where data for government corruption are missing, the perception of business corruption is used as the overall corruption-perception measure.

#### Continuous Independent Variables:

+ **Regional Indicator:** Region in the world, made of 10 levels: Western Europe, Central and Eastern Europe, Sub-Saharan Africa, Middle East and North Africa, East Asia, Southeast Asia, South Asia, North America and ANZ, Latin America, Caribbean and Commonwealth of Independent States.

First, we read both the train (2018) and test (2019) datasets and we'll take a look on the train dataset summary.

```{r}
#Train Dataset
#"WHR2018.csv"
#"WHR2019.csv"
train <- read.csv("WHR2018.csv", header = TRUE, sep = ",", dec=".",na.strings="N/A", row.names=2)
raw_dataset <- train

#Test Dataset
test <- read.csv("WHR2019.csv", header = TRUE, sep = ",", dec=".",na.strings="N/A", row.names=2)

summary(train)
```

We can also have a sample of the most and least happiest countries:
```{r echo=FALSE}
head(train,3)
tail(train,3)
```

### Pre-processing

From the summary of the dataset we can see that Overall.rank represents the Happiest ranking in terms of the Score, so we have proceeded to remove the Overall.rank variable.

We have created two versions of the train dataset. The former includes the Regional.Indicator to be used in terms of visualizations, while the latter does not contain this variable in order to work with numerical variables only.

```{r}
#Removing Overall.rank and Regional.Indicator
PCA_train <- train[-c(1)]
train <- train[-c(1,9)]

#Removing Overall.rank
test <- test[-c(1)]
```

### Missing Values

There exists one missing value on the Perceptions.of.corruption variable in the train dataset, the one has been imputed.

```{r Remove 1 NA, include=FALSE}
## Impute Missing Value
impute <- mice(train, m=5, seed=500)
train <- mice::complete(impute, 1)
row.names(train) <- row.names(raw_dataset)
```

### Outliers

We have also detected four outliers. Three of them: Denmark, Singapore, and Rwanda, were detected as univariate outliers in terms of the Perceptions.of.corruption variable. While the fourth: United Arab Emirates was considered an outlier given the Classical Mahalanobis distance.

```{r echo=FALSE}
uni_outliers <- ggplot(train, aes(x=as.factor(1), y=Perceptions.of.corruption)) 
uni_outliers <- uni_outliers + xlab("Extreme Univariate Outliers on Perceptions.of.corruption") 
uni_outliers <- uni_outliers + geom_boxplot(outlier.colour="black", outlier.shape=1, coef=3)
uni_outliers <- uni_outliers + theme(legend.position = "none")
uni_outliers <- uni_outliers + geom_text(data = train[c(3,34,151),],
                                         aes(x = as.factor(1), 
                                             y = Perceptions.of.corruption),
                                         label=row.names(train[c(3,34,151),]),
                                         col='black', size=4.5)
uni_outliers
```


```{r echo=FALSE}
#Multivariate Outliers with Clasical Mahalanobis Distance
outliers <- Moutlier(train[-c(3,34,151),-c(8)], quantile = 0.999, plot = F)
densityplot(outliers$md, xlab="Classical Distances")
```

```{r PCA, include=FALSE}
#Removing Outliers on the datasets
outliers <- c('Denmark','Singapore', 'Rwanda', "United Arab Emirates") # 3,20,34,151
train <- train[!row.names(train)%in%outliers,]
raw_dataset <- raw_dataset[!row.names(raw_dataset)%in%outliers,]
```

### Feature selection

We would like to take a last look at our variables in order to select those that help us to reduce overfitting, improve accuracy, or reduce training time.

We observed that GDP.per.capita is the variable with highest correlation with Happiness Score with a value of 0.80 in Pearson’s correlation, followed by Health.life.expeactancy, so countries with a higher GDP per capita seem to be happier, yet this time we have decided to maintain our actual variables and finish our pre-processing.

```{r echo=FALSE}
chart.Correlation(train, histogram=TRUE)
```

```{r include=FALSE}
#Cleanup
#Removing variables that we're not longer gonna use
rm(impute,outliers,uni_outliers)
```

***
## Visualization and Clustering

In order to continue our data exploration, this time we would like to visualize it and create different clusters of countries according to the available variables.

### Hapiness Score by Region

First, we can take a look of the Happiness Score per Regional.Indicator, where we can observe that the North America and ANZ are the most happiest countries in the World, followed by Western Europe and Latin America and Caribbean, while the least happiest countries are located in Sub-Saharan Africa Region.

```{r echo=FALSE}
#Boxplot
regional_boxplot <- ggplot(raw_dataset, aes(x = Regional.Indicator, y = Score, color = Regional.Indicator))
regional_boxplot <- regional_boxplot + geom_boxplot()
regional_boxplot <- regional_boxplot + ggtitle("Boxplot of the Happiness Score per Regional.Indicator")
regional_boxplot <- regional_boxplot + theme(legend.position="none")
regional_boxplot <- regional_boxplot + coord_flip()
regional_boxplot
rm(regional_boxplot)
```

### K-means Clustering

Now, we would like to create new clusters, different from the Regional.Indicator, so this time we’re working with our numerical matrix in order to discover new groups of countries.

```{r include=FALSE}
#We set the seed for reproducibility
set.seed(119)
```

First, we try a K-means clustering technique with K=3. We can see the change in the coordinates of the centroids, from their first position with black squares to their final position in colored circles. We can clearly see the correlations of some variables when we see the clusters of countries grouped to each other.

```{r echo=FALSE}
#K-means
kmeans.3 <- cclust(as.matrix(train), 3, iter.max=100, method="kmeans", dist="euclidean")
```

```{r echo=FALSE}
par(mfrow=c(1,2))
#Plot
plot(train$Score,train$GDP.per.capita,col=(kmeans.3$cluster+1), xlab="Happiness Score", ylab="GDP.per.capita")
points(kmeans.3$initcenters, pch = 15)
points(kmeans.3$centers, pch = 17, col=c(2,3,4), cex=2)
arrows (kmeans.3$initcenters[,1], kmeans.3$initcenters[,2], kmeans.3$centers[,1], kmeans.3$centers[,2])
points(kmeans.3$centers,bg=seq(1:kmeans.3$ncenters)+1,cex=2,pch=21,col='black')

#Plot
plot(train$Score,train$Social.support,col=(kmeans.3$cluster+1), xlab="Happiness Score", ylab="Social.support")
points(kmeans.3$initcenters, pch = 15)
points(kmeans.3$centers, pch = 17, col=c(2,3,4), cex=2)
arrows (kmeans.3$initcenters[,1], kmeans.3$initcenters[,2], kmeans.3$centers[,1], kmeans.3$centers[,2])
points(kmeans.3$centers,bg=seq(1:kmeans.3$ncenters)+1,cex=2,pch=21,col='black')
```

Clustering quality can be measured by the Calinski-Harabasz index, so we would like to know if K=3 is the best parameter to our dataset.

From the plot, we observe that the highest, i.e, the best Calinski-Harabasz index value corresponds to the K=3.

```{r include=FALSE}
#We set the seed for reproducibility
set.seed(119)
```

```{r echo=FALSE}
ch_values <- 2:15
for (K in 2:15) {
  kmeans <- cclust(as.matrix(train), K, iter.max=100, method="kmeans", dist="euclidean")
  ch_values[K-1] <- clustIndex(kmeans, train, index="calinski")
}
plot(2:15, ch_values, type="b", ylab="Calinski index", xlab="K")
```

### PCA Interpretation with Dimensionality Reduction

Sometimes it can be messy to deal with a large number of variables, so we have reduced the dimensionality of our dataset through PCA. The seven original variables can be reduced to 2 of them, holding for ~73% of the original information.

```{r echo=FALSE}
#PCA
res.pca <- PCA(train, quanti.sup = 1,  scale.unit = TRUE, graph = F)

#Check Eigenvalues
res.pca$eig

#2 Significant dimenions holding up to ~73% of the original information
significant_dimensions <- 2

#Merging coordinates of individuals
new_train <- res.pca$ind$coord[,1:significant_dimensions]
new_train <- data.frame(new_train)
```

Now, our new dataset looks like this:

```{r echo=FALSE}
head(new_train,3)
```

We have repeated the K-means grid-search in order to check if now there exists a new optimum K, yet K=3 it's once again the optimal value for clustering.

```{r include=FALSE}
#We set the seed for reproducibility
set.seed(119)
```

```{r echo=FALSE}
ch_values <- 2:15
for (K in 2:15) {
  kmeans <- cclust(as.matrix(new_train), K, iter.max=100, method="kmeans", dist="euclidean")
  ch_values[K-1] <- clustIndex(kmeans, new_train, index="calinski")
}
plot(2:15, ch_values, type="b", ylab="Calinski index", xlab="K")
```

### EM Clustering

We can also use this modified training dataset with two dimensions to run another kind of clustering technique. EM Clustering is similar to the K-Means technique, but its main goal is to maximize the overall probability or likelihood of the data given the clusters, so it allows us to compute new and interesting plots, as the possible former gaussians of the countries.

We ran a 10 clusters grid search, getting K=3 as the optimal cluster numbers.

```{r include=FALSE}
#We set the seed for reproducibility
set.seed(119)
```

```{r echo=FALSE}
EM <- mixmodCluster (new_train, nbCluster = 2:10)
summary(EM)
```

```{r include=FALSE}
#We set the seed for reproducibility
set.seed(119)
```

```{r echo=FALSE}
mixture_model <- mixmodCluster(new_train,3)
plotCluster(mixture_model["bestResult"], new_train)
hist(mixture_model)
```

```{r include=FALSE}
#Cleanup
#Removing variables that we're not longer gonna use
rm(K,ch_values,significant_dimensions,mixture_model,EM,kmeans, kmeans.3, res.pca)
```

We have completed our visualization and clustering analysis.

***
## Machine Learning Modeling
Machine learning methods allow us to create different predictive models, yet choosing the appropriate one it’s not always a trivial task. Looking for the best model we have considered different options, including linear and non-linear methods, such as linear and polynomial regressions, decision trees, random forest, and neural networks.

The different models were built with the 2018 Happiness Ranking dataset and were first evaluated using different measurement techniques. In regression we run a 10-cross fold validation resampling procedure over the training dataset, the one was splitted 70%-30 for train and test, evaluating and comparing the regression models through the Normalized Mean Squared Error, NMSE. In decision trees we have also used a cross-validation error function, while in random forests we have the OOB estimate of  error rate. Finally, in neural networks we ran a 10-cross fold validation and evaluated the Root Mean Squared Error, RMSE.

Having different measurement procedures for the models makes it difficult to fairly compare their performance, so the best model has been chosen through the prediction error for the 2019 Happiness Score values and comparing them with their real values by means of the Root Mean Squared Error, RMSE.


We observed that different runs of neural networks for training may generate different solutions. In short, in some runs the neural networks were the best models, narrowly overcoming random forests, yet sometimes their performance wasn’t that good, being random forests the best model. In terms of the regressions, the linear standard regression was the one with better performance. Decision trees on the other side did not perform as expected.


#####  A.- Linear Regression 

To evaluate the different regression models we selected the best parameterization for linear regression, ridge and lasso regularized linear regressions, and polynomial regression, and then ran a 10 cross fold validation in order to compute NMSE. We describe how we got the best parameters for different regression models and then we compare their results.

#####  A.1.- Standart LR

A model with the lm()function was built, on which a backward stepwise feature selection was performed in order to find not useful variables for the model.

From our data, the only variable that happened to reduce the AIC was Generosity, which was also the only variable that was not significantly correlated with the Happiness Score (with a Pearson’s correlation of 0.12). Therefore, when comparing models, a standard linear regression without this variable was performed. 

```{r glm Model, echo=FALSE}
model1 <- lm(Score ~ ., data=train)
(model1.sel <- step(model1)) # Backward Step-Wise Feature Selection (Based on AIC)
```

##### A.2.- Ridge Linear Regression 

Ridge regression is a regularized regression with the coefficients penalized by the L2 norm. In order to find the optimal regularization parameter λ, the function lm.ridge() has the built in cross validation option to find the best one. The best λ for this model is 17.3.

```{r echo=FALSE}
model.ridge <- lm.ridge(Score ~ ., data=train, lambda=seq(0,34,0.1))

plot(seq(0,34,0.1), model.ridge$GCV, main="GCV of Ridge Regression", type="l", 
     xlab=expression(lambda), ylab="GCV")

# The optimal lambda is given by
lambda.ridge <- seq(0,34,0.1)[which.min(model.ridge$GCV)]
abline (v=lambda.ridge,lty=2)
```

##### A.3.- LASSO

In Lasso regression, the coefficients are penalized by the L1 norm and for this reason, unlike ridge regression, the slope of the coefficients can reach 0 and consequently some variables can be excluded from the model.  When the Lasso regression was built with the glmnet() function, considering all variables, the model did not take into account the Generosity variable, coinciding with the results of the stepwise feature selection.  Like ridge, the optimal value for λ was chosen by cross-validation, which in this model was 0.02126.

```{r echo=FALSE}
set.seed(119)
x_vars <- model.matrix(Score~., train)[,-1]
y_var <- train$Score
x_test <- model.matrix(Score~., test)[,-1]
model.lasso <- cv.glmnet(x_vars,y_var,nfolds=10)
coef(model.lasso) # Generosity is discarted by the model
plot(model.lasso)

#Lambda
lambda.lasso <- model.lasso$lambda.min # Gives the minimum cross validated error
predicts.lasso <- predict(model.lasso, newx = x_test, s= "lambda.min")
```

##### A.4 - Polynomial Regression

In polynomial regression, the best nth degree polynomial in x that best fitted our data was found by means of 10-fold cross validation. The degrees from 2 to 10 were considered, being the 6th degree the one that gave the minimum cross validation error.

```{r echo=FALSE}
set.seed(119)

mse <- rep(NA, 10)
for (i in 1:10) {
    fit <- glm(Score ~ poly(GDP.per.capita + Social.support + Healthy.life.expectancy + Freedom.to.make.life.choices + Generosity + Perceptions.of.corruption),data=train)
    mse[i] <- cv.glm(train, fit, K=10)$delta[1]
}
plot(1:10, mse, xlab = "Degree", ylab = "Validation MSE", type = "l")
d.min <- which.min(mse)
points(which.min(mse), mse[which.min(mse)], col = "orangered2", cex = 1, pch = 20)
abline (v=which.min(mse),lty=2)
```

##### A.5.- Comparison of models

To select the best model, a 10-fold cross validation was performed, using as λ’s in ridge and lasso the ones found previously, the 6th degree in the polynomial regression, and removing the Generosity variable in the standard linear regression.

```{r echo=FALSE}
set.seed(119)
data <- train # by just in case

K <- 10; TIMES <- 10   # 10x10-cv

res <- replicate (TIMES, {
  # shuffle the data
  data <- data[sample(nrow(data)),]
  # Create K equally sized folds
  folds <- cut (1:nrow(data), breaks=K, labels=FALSE)
  sse.standard <- sse.ridge <- sse.lasso <- sse.poly <-  0

  # Perform 10 fold cross validation
  for (i in 1:K)
  {
    valid.indexes <- which (folds==i, arr.ind=TRUE)
    valid.data <- data[valid.indexes, ]
    train.data <- data[-valid.indexes, ]

    #standard
    model.standard <- lm (Score ~ .-Generosity, data=train.data)
    beta.standard <- coef(model.standard)
    preds.standard <- beta.standard[1] + as.matrix(valid.data[,c(2:5,7)]) %*% beta.standard[2:6]
    sse.standard <- sse.standard + crossprod(valid.data[,'Score'] - preds.standard)

    #ridge
    model.ridgereg <- lm.ridge (Score ~ ., data=train.data, lambda = lambda.ridge)
    beta.ridgereg <- coef (model.ridgereg)
    preds.ridgereg <- beta.ridgereg[1] + as.matrix(valid.data[,c(2:7)]) %*% beta.ridgereg[2:7]
    sse.ridge <- sse.ridge + crossprod(valid.data[,'Score'] - preds.ridgereg)

    #lasso
    model.lasso <- glmnet(as.matrix(train.data[,2:7]), as.numeric(train.data[,'Score']),   lambda=lambda.lasso)
    preds.lasso <- predict(model.lasso, newx = as.matrix(valid.data[,2:7]), s = lambda.lasso)
    sse.lasso <- sse.lasso + crossprod(valid.data[,'Score'] - preds.lasso)
    
    #ploynomial
    model.poly <- glm(Score ~ poly(GDP.per.capita + 
                                     Social.support + 
                                     Healthy.life.expectancy + 
                                     Freedom.to.make.life.choices + 
                                     Generosity +
                                     Perceptions.of.corruption, 
                                     5, raw=TRUE), data=train.data, family = gaussian) 
    preds.poly <- predict(model.poly, newdata=valid.data[,2:7])
    sse.poly <- sse.poly + crossprod(valid.data[,'Score'] - preds.poly)
    
  }
  c(sse.standard, sse.ridge, sse.lasso, sse.poly)
})

normalization <- (nrow(train)-1)*var(train$Score) # denominator of NRMSE
nmse.train <- rowMeans(res) / normalization
errors <- (as.data.frame(nmse.train))
rownames(errors) <- c("Standard", "Ridge", "Lasso","Poly") ; colnames(errors) <- "NMSE train"
errors 
```

We chose the best model as the one with the lowest CV error, which turns out to be the standard linear regression.

#####  B.- Decision Trees

Decision trees are easy to interpret and understand, yet their performance could be far from expected. Random forests on the other hand combine multiple decision trees, so it becomes more difficult to interpret, yet they obtained a great performance when predicting the 2019 World Happiness Score.

```{r include=FALSE}
#We set the seed for reproducibility
set.seed(119)
```

```{r Decision Tree, include=FALSE}
#Decision Tree
df.tree <- rpart(Score ~ ., data=train,method="anova",control=rpart.control(cp=0.001, xval=10))
#Post Prunning
table_tree <- as.data.frame(df.tree$cptable)
min_error_tree <- which.min(table_tree$xerror)
alpha = table_tree$CP[min_error_tree]
optimum_tree <- prune(df.tree,cp=alpha)
```

```{r Random Forest, include=FALSE}
#Random Forest
rf <- randomForest(formula = Score ~ .,data = train, type="regression", importance=TRUE)
```

When testing the models with the 2019 World Ranking Happiness dataset we can observe how far are the data points respect to their real values on the decision trees models, while Random forests kind of approximate to a straight line. 
```{r echo=FALSE}
#Predict for 2019
decisionTree_predict <- predict(optimum_tree,newdata = test,type = "vector")
randomForest_predict <- predict(object = rf,newdata = test)

#Decision Tree
decisionTreeComparission <- as.data.frame(cbind(test[,'Score'], as.vector(decisionTree_predict)))
colnames(decisionTreeComparission) <- c("Actual","Predicted")

#Random Forest
rfComparission <- as.data.frame(cbind(test[,'Score'], as.vector(randomForest_predict)))
colnames(rfComparission) <- c("Actual","Predicted")

dt.plot <- ggplot(decisionTreeComparission, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Decision Tree", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

rf.plot <- ggplot(rfComparission, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Random Forest", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

gridExtra::grid.arrange(dt.plot,rf.plot, nrow=1, ncol=2)
```

#####  C.- Neural Network

Neural networks provide a non-linear method to predict new scores. One important thing about neural networks it’s about their hyper-parametrization regarding the number of hidden layers, and its size. 

For the purpose of our investigation, we tried with two configurations over a single hidden layer neural network. In the first configuration (nn1), we run a grid search looking for the optimal size of the hidden layer, which it was two. The second configuration (nn2) was another grid search trying different decay values in order to avoid over-fitting.

For both configurations we built the models pre-scaling the data and evaluating them through 10 cross fold validation with the 2018 Happiness dataset.

Different runs of the neural networks turned into different results, yet our first configuration (nn1) with one hidden layer of size two and no regularization turned out to be the best model, i.e, the one with the smaller error.


```{r include=FALSE}
#We set the seed for reproducibility
set.seed(75)
```

```{r include=FALSE}
#First configuration: nn1
sizes <- seq(1,10,by=1)
trc <- trainControl (method="repeatedcv", number=10, repeats=10)

nn1<- train (Score ~., data = train,
                        method='nnet',
                        maxit = 500,
                        trace = FALSE,
                        tuneGrid = expand.grid(.size=sizes,.decay=0), 
                        preProcess = c("scale"),
                        trControl=trc,
                        linout = TRUE)

```

Best neural network model:

```{r echo=FALSE}
nn1
plotnet(nn1)
```

```{r include=FALSE}
#Second configuration: nn2
decays <- 10^seq(-2, 0, by=0.2)
lay <- seq(1,5,1)
nn2 <- train (Score ~., data = train,
                        method='nnet',
                        maxit = 500,
                        trace = FALSE,
                        tuneGrid = expand.grid(.size=2,.decay=decays), 
                        preProcess = c("scale"),
                        trControl=trc,
                        linout = TRUE)
```

***
##  Final Model

In order to select the best model we have evaluated their performance using the 2019 World Ranking Happiness Score and compare the predictions with the real values through Root Mean Squared Error, RMSE.

So far, neural networks and random forest were the best models, yet neural networks performed a little better, with 0.4818 RMSE value.

```{r echo=FALSE}
#Standard Regression Predict for 2019
model.standard <- lm (Score ~ .-Generosity, data=train)
beta.standard <- coef(model.standard)
preds.standard <- beta.standard[1] + as.matrix(test[,c(2:5,7)]) %*% beta.standard[2:6]
#Standard Regression RMSE
standard_MSE <- Metrics::mse(test$Score, preds.standard)
standard_RMSE <- Metrics::rmse(test$Score, preds.standard)

#Ridge Predict for 2019
#model.ridgereg
model.ridgereg <- lm.ridge (Score ~ ., data=train, lambda = lambda.ridge)
beta.ridgereg <- coef (model.ridgereg)
preds.ridgereg <- beta.ridgereg[1] + as.matrix(test[,c(2:7)]) %*% beta.ridgereg[2:7]
#Standard Regression RMSE
ridgereg_MSE <- Metrics::mse(test$Score, preds.standard)
ridgereg_RMSE <- Metrics::rmse(test$Score, preds.standard)

#Lasso Regression Predict for 2019
#model.lasso
preds.lasso <- predict(model.lasso, newx = as.matrix(test[,2:7]), s = lambda.lasso)
#Standard Regression RMSE
lasso_MSE <- Metrics::mse(test$Score, preds.lasso)
lasso_RMSE <- Metrics::rmse(test$Score, preds.lasso)

#Poly Regression Predict for 2019
#model.poly
model.poly<- glm(Score ~ poly(GDP.per.capita + Social.support + Healthy.life.expectancy + Freedom.to.make.life.choices + Generosity +  Perceptions.of.corruption,6, raw=TRUE), data=train, family = gaussian)
preds.poly <- predict(model.poly, newdata=test[,2:7])
#Standard Regression RMSE
poly_MSE <- Metrics::mse(test$Score, preds.poly)
poly_RMSE <- Metrics::rmse(test$Score, preds.poly)

#Decision Tree
#Predict for 2019
decisionTree_predict <- predict(optimum_tree,newdata = test,type = "vector")
#Decision Tree RMSE
decisionTree_MSE <- Metrics::mse(test$Score, decisionTree_predict)
decisionTree_RMSE <- Metrics::rmse(test$Score, decisionTree_predict)

#Random Forest
#Predict for 2019
randomForest_predict <- predict(object = rf,newdata = test)
#Random Forest RMSE
randomForest_MSE <- Metrics::mse(test$Score, randomForest_predict)
randomForest_RMSE <- Metrics::rmse(test$Score, randomForest_predict)

#nn1
#Predict for 2019
nn1_predict <- predict(nn1, newdata = test)
#nn1 RMSE
nn1_mse <- Metrics::mse(test$Score, nn1_predict)
nn1_rmse <- Metrics::rmse(test$Score, nn1_predict)

#nn2
#Predict for 2019
nn2_predict <- predict(nn2, newdata = test)
#nn2 RMSE
nn2_mse <- Metrics::mse(test$Score, nn2_predict)
nn2_rmse <- Metrics::rmse(test$Score, nn2_predict)

#Summary RMSE Table
rowtab <- t(as.data.frame(c(standard_RMSE,ridgereg_RMSE,lasso_RMSE,poly_RMSE,decisionTree_RMSE,randomForest_RMSE,nn1_rmse,nn2_rmse)))
errors <- rbind(rowtab)
rownames(errors) <- c("RMSE")
colnames(errors) <- c("Standard","Ridge","Lasso","Polynomial","decisionTree","randomForest","nn1","nn2")
data.frame(t(errors))

```


It is also interesting to visualize the plots of the actual vs predicted happiness scores of the 2019 dataset, where we can observe how the NN1 model has the better fit, while in the decision tree model the predicted scores are far away from their real values.

```{r PLOT, echo=FALSE}
#Standard
std_vs <- as.data.frame(cbind(test[,'Score'], as.vector(preds.standard)))
colnames(std_vs) <- c("Actual","Predicted")
std_plot <- ggplot(std_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Standard", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

#Ridge
ridge_vs <- as.data.frame(cbind(test[,'Score'], as.vector(preds.ridgereg)))
colnames(ridge_vs) <- c("Actual","Predicted")
ridge_plot <- ggplot(ridge_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Ridge", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

#Lasso
lasso_vs <- as.data.frame(cbind(test[,'Score'], as.vector(preds.lasso)))
colnames(lasso_vs) <- c("Actual","Predicted")
lasso_plot <- ggplot(lasso_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Lasso", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

#Poly
poly_vs <- as.data.frame(cbind(test[,'Score'], as.vector(preds.poly)))
colnames(poly_vs) <- c("Actual","Predicted")
poly_plot <- ggplot(poly_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Poly", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

#Decision Tree
dt_vs <- as.data.frame(cbind(test[,'Score'], as.vector(decisionTree_predict)))
colnames(dt_vs) <- c("Actual","Predicted")
dt_plot <- ggplot(dt_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Decision Tree", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

#Random Forest
rf_vs <- as.data.frame(cbind(test[,'Score'], as.vector(randomForest_predict)))
colnames(rf_vs) <- c("Actual","Predicted")
rf_plot <- ggplot(rf_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "Random Forest", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

#nn1
nn1_vs <- as.data.frame(cbind(test[,'Score'], as.vector(nn1_predict)))
colnames(nn1_vs) <- c("Actual","Predicted")
nn1_plot <- ggplot(nn1_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "nn1", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))

#nn2
nn2_vs <- as.data.frame(cbind(test[,'Score'], as.vector(nn2_predict)))
colnames(nn2_vs) <- c("Actual","Predicted")
nn2_plot <- ggplot(nn2_vs, aes(Actual, Predicted )) + 
geom_point(cex=0.7) + theme_bw() + geom_abline() +
labs(title = "nn2", x = "Actual happiness score",
y = "Predicted happiness score") +
theme(plot.title = element_text(face = "bold", size = (15), hjust=0.5),
axis.title = element_text(size = (10)))


#Plot
gridExtra::grid.arrange(std_plot,ridge_plot,lasso_plot,poly_plot,dt_plot,rf_plot,nn1_plot,nn2_plot,
                        nrow=2, ncol=4)

```

***
##  Conclusions

We have studied the World Happiness Report through different Machine Learning techniques. It is interesting to analyze the countries of the world through such relevant variables  as GDP.per.capita, which it turned to be one of the most correlated variables with the national Happiness Score.

We can also visualize and cluster the countries in different groups. Three clusters of countries were clearly defined and segmented by some correlated variables as their GDP.per.capita.  

Regarding the machine learning methods we have observed the power and flexibility of non-linear models such as neural networks random forests. While neural networks are often excellent choices as a model, it is still important to know how they work and be minded that different runs can produce different results, and some of them didn't were better than random forest, which is less computationally expensive than neural networks. So far, our dataset was a little small and neural networks will require more data to build a robust model.

It will be interesting to try and build new models in the future in order to predict the World Happiness Score in order to help in building a better and happier society.

